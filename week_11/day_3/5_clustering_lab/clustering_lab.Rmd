---
title: "Decision tree & clustering lab"
output: html_document
---
```{r}
library(rpart)
library(rpart.plot)
library(dplyr)
```

```{r}
avocado
```


Build a decision tree to model the likelihood of a sale being of an organic avocado (and then test how well your model performs)


```{r}
head(avocado)
```
```{r}
library(lubridate)
library(janitor)
library(tidyverse)
```

```{r}
trimmed_avocados <- avocado %>%
  mutate(
    quarter = as_factor(quarter(Date)),
    year = as_factor(year),
    type = as_factor(type)
  ) %>%
  select(-c("X1", "Date"))
```
SALE : ORGANIC
```{r}
trimmed_avocados
```


```{r}
n_data <- nrow(trimmed_avocados)
test_index <- sample(1:n_data, size = n_data*0.2)
avocado_test <- slice(trimmed_avocados, test_index)
avocado_train <- slice(trimmed_avocados, -test_index)
```

```{r}
avocado_test %>%
 janitor::tabyl(type)
```
```{r}
avocado_train %>%
 janitor::tabyl(type)
```

```{r}
avocado_fit <- rpart(type ~ ., 
                     data = avocado_train, 
                     method = 'class')

rpart.plot(avocado_fit, yesno = 2)
```

```{r}
rpart.rules(avocado_fit, cover = TRUE)
```

CLUSTERING:



```{r}
comp <- read.csv("computers.csv")
comp
```
hd, ram


```{r}
comp <- comp %>%
              janitor::clean_names() 

comp
```

```{r}
any (is.na(comp))
```


```{r}
comp <- comp  %>% 
              select(c(ram, hd))

head(comp)
```
```{r}
summary(comp)
```

```{r}
comp_scale <- comp %>%
                      mutate_all(scale)

comp_scale
```




```{r}
ggplot(comp_scale, aes(ram, hd)) +
  geom_point()
```



k -> no of centroid 
```{r}
set.seed(1234)
clustered_comp <- kmeans(comp_scale, centers = 6, nstart = 25)
clustered_comp
```

```{r}
library(broom)

tidy(clustered_comp, 
     col.names = colnames(comp_scale))
```

```{r}
glance(clustered_comp)
```

```{r}
augment(clustered_comp, comp)
```

```{r}
library(animation)

comp_scale %>% 
  kmeans.ani(centers = 6) 
```

```{r}
glance(clustered_comp)
```


ELBOW POINT
```{r}
clusterings <- k_clusters %>%
  unnest(glanced)
clusterings
```



```{r}
max_k <- 20
k_clusters <- tibble(k = 1:max_k) %>%
  mutate(kclust = map(k, ~kmeans(comp_scale, .x, nstart = 25)),
                      tidied = map(kclust, tidy),
                      glanced = map(kclust, glance),
                      augment = map(kclust, augment , comp)
                      )
  k_clusters
```

```{r}
clusterings <- k_clusters %>%
  unnest(glanced)
clusterings
```

```{r}
ggplot(clusterings, aes(x=k, y=tot.withinss)) +
  geom_point() +
    geom_line() +
    scale_x_continuous(breaks = seq(1, 20, by = 1))
```

```{r}
library(factoextra)
```

```{r}
fviz_nbclust(comp_scale, kmeans, method = "wss", nstart = 25)
```
Silhouette coefficient
```{r}
fviz_nbclust(comp_scale, kmeans, method = "silhouette", nstart = 25)
```



Gap statistic method: choosing clusters
```{r}
library(cluster)
fviz_nbclust(comp_scale, kmeans, method = "gap_stat", nstart = 25, k.max = 5)
```
k = 5 is most optimal number of clusters using gap statistic.



Visualizing:


```{r}
clusterings %>% 
  unnest(cols = c(augment)) %>%
  filter(k <= 5) %>%
 ggplot(aes(x = ram, y = hd)) +
  geom_point(aes(color = .cluster)) + 
  facet_wrap(~ k)
```

```{r}
clusterings %>% 
  unnest(cols = c(augment)) %>%
  filter(k == 5) %>%
 ggplot(aes(x = ram, y = hd, colour = .cluster)) +
  geom_point(aes(color = .cluster)) +
  geom_text(hjust = 0, vjust = - 0.5, size = 3)
```

